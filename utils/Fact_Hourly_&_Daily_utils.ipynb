{"cells":[{"cell_type":"code","source":["%sql\ndrop table if exists silver_processed_fact_df;\n     "],"metadata":{"application/vnd.databricks.v1+cell":{"showTitle":false,"cellMetadata":{"rowLimit":10000,"byteLimit":2048000,"implicitDf":true},"nuid":"de6c241c-e57e-4038-bd12-04309362bb35","inputWidgets":{},"title":""}},"outputs":[{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"overflow":false,"datasetInfos":[],"data":[],"plotOptions":{"displayType":"table","customPlotOptions":{},"pivotColumns":null,"pivotAggregation":null,"xColumns":null,"yColumns":null},"columnCustomDisplayInfos":{},"aggType":"","isJsonSchema":true,"removedWidgets":[],"aggSchema":[],"schema":[],"aggError":"","aggData":[],"addedWidgets":{},"metadata":{},"dbfsResultPath":null,"type":"table","aggOverflow":false,"aggSeriesLimitReached":false,"arguments":{}}},"output_type":"display_data","data":{"text/html":["<style scoped>\n","  .table-result-container {\n","    max-height: 300px;\n","    overflow: auto;\n","  }\n","  table, th, td {\n","    border: 1px solid black;\n","    border-collapse: collapse;\n","  }\n","  th, td {\n","    padding: 5px;\n","  }\n","  th {\n","    text-align: left;\n","  }\n","</style><div class='table-result-container'><table class='table-result'><thead style='background-color: white'><tr></tr></thead><tbody></tbody></table></div>"]}}],"execution_count":0},{"cell_type":"code","source":["def join_data(silver_df, date_df, time_df):\n    from pyspark.sql.functions import hour, col\n    joined_df = silver_df.join(date_df, date_df['fullDate'] == silver_df['date'].cast('date'), 'inner')\n    joined_df = joined_df.join(time_df, hour(joined_df['created_on']) == time_df['timeID'], 'inner')\n    \n    selected_columns = [\"dateID\", \"timeID\", \"city_id\", \"dt\", \"temp\", \"temp_min\", \"temp_max\", \"visibility\", \"pressure\", \"humidity\", \"wind_deg\", \"wind_gust\", \"created_on\"]\n    new_silver_df = joined_df.select(*selected_columns)\n    \n    new_silver_df.orderBy(col(\"created_on\").desc()).display()\n    return new_silver_df\n"],"metadata":{"application/vnd.databricks.v1+cell":{"showTitle":false,"cellMetadata":{"rowLimit":10000,"byteLimit":2048000},"nuid":"25e8b34c-a20c-42ff-95ac-fc8b86634f97","inputWidgets":{},"title":""}},"outputs":[],"execution_count":0},{"cell_type":"code","source":["@keep_log\ndef hourly_fact(new_silver_df):\n    from datetime import datetime\n    from pyspark.sql.functions import col\n    df = new_silver_df.dropDuplicates(['city_id','timeID','dateID']).orderBy(col(\"created_on\").desc())\n    start = datetime.fromtimestamp(df.selectExpr(\"min(dt)\").first()[0])\n    end = datetime.fromtimestamp(df.selectExpr(\"max(dt)\").first()[0])\n    return df,start,end\n\n"],"metadata":{"application/vnd.databricks.v1+cell":{"showTitle":false,"cellMetadata":{"rowLimit":10000,"byteLimit":2048000},"nuid":"ee4b5b71-5175-44bb-ab8c-773913781959","inputWidgets":{},"title":""}},"outputs":[{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"data":"\u001B[0;31m---------------------------------------------------------------------------\u001B[0m\n\u001B[0;31mNameError\u001B[0m                                 Traceback (most recent call last)\nFile \u001B[0;32m<command-3800322817382674>:1\u001B[0m\n\u001B[0;32m----> 1\u001B[0m \u001B[38;5;129m@keep_log\u001B[39m\n\u001B[1;32m      2\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21mhourly_fact\u001B[39m(new_silver_df):\n\u001B[1;32m      3\u001B[0m     \u001B[38;5;28;01mfrom\u001B[39;00m \u001B[38;5;21;01mdatetime\u001B[39;00m \u001B[38;5;28;01mimport\u001B[39;00m datetime\n\u001B[1;32m      4\u001B[0m     df \u001B[38;5;241m=\u001B[39m new_silver_df\u001B[38;5;241m.\u001B[39mdropDuplicates([\u001B[38;5;124m'\u001B[39m\u001B[38;5;124mcity_id\u001B[39m\u001B[38;5;124m'\u001B[39m,\u001B[38;5;124m'\u001B[39m\u001B[38;5;124mtimeID\u001B[39m\u001B[38;5;124m'\u001B[39m,\u001B[38;5;124m'\u001B[39m\u001B[38;5;124mdateID\u001B[39m\u001B[38;5;124m'\u001B[39m])\u001B[38;5;241m.\u001B[39morderBy(col(\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mcreated_on\u001B[39m\u001B[38;5;124m\"\u001B[39m)\u001B[38;5;241m.\u001B[39mdesc())\n\n\u001B[0;31mNameError\u001B[0m: name 'keep_log' is not defined","errorSummary":"<span class='ansi-red-fg'>NameError</span>: name 'keep_log' is not defined","metadata":{},"errorTraceType":"ansi","type":"ipynbError","arguments":{}}},"output_type":"display_data","data":{"text/plain":["\u001B[0;31m---------------------------------------------------------------------------\u001B[0m\n","\u001B[0;31mNameError\u001B[0m                                 Traceback (most recent call last)\n","File \u001B[0;32m<command-3800322817382674>:1\u001B[0m\n","\u001B[0;32m----> 1\u001B[0m \u001B[38;5;129m@keep_log\u001B[39m\n","\u001B[1;32m      2\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21mhourly_fact\u001B[39m(new_silver_df):\n","\u001B[1;32m      3\u001B[0m     \u001B[38;5;28;01mfrom\u001B[39;00m \u001B[38;5;21;01mdatetime\u001B[39;00m \u001B[38;5;28;01mimport\u001B[39;00m datetime\n","\u001B[1;32m      4\u001B[0m     df \u001B[38;5;241m=\u001B[39m new_silver_df\u001B[38;5;241m.\u001B[39mdropDuplicates([\u001B[38;5;124m'\u001B[39m\u001B[38;5;124mcity_id\u001B[39m\u001B[38;5;124m'\u001B[39m,\u001B[38;5;124m'\u001B[39m\u001B[38;5;124mtimeID\u001B[39m\u001B[38;5;124m'\u001B[39m,\u001B[38;5;124m'\u001B[39m\u001B[38;5;124mdateID\u001B[39m\u001B[38;5;124m'\u001B[39m])\u001B[38;5;241m.\u001B[39morderBy(col(\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mcreated_on\u001B[39m\u001B[38;5;124m\"\u001B[39m)\u001B[38;5;241m.\u001B[39mdesc())\n","\n","\u001B[0;31mNameError\u001B[0m: name 'keep_log' is not defined"]}}],"execution_count":0},{"cell_type":"code","source":["@keep_log\ndef daily_data(new_silver_df):\n    from pyspark.sql.window import Window\n    from pyspark.sql.functions import avg\n    from datetime import datetime\n\n    window_spec = Window.partitionBy(new_silver_df.dateID)\n    avg_weather_data = new_silver_df.withColumn(\"avg_temperature\", avg(new_silver_df.temp).over(window_spec)) \\\n                              .withColumn(\"avg_temp_min\", avg(new_silver_df.temp_min).over(window_spec)) \\\n                              .withColumn(\"avg_temp_max\", avg(new_silver_df.temp_max).over(window_spec)) \\\n                              .withColumn(\"avg_pressure\", avg(new_silver_df.pressure).over(window_spec)) \\\n                              .withColumn(\"avg_visibility\", avg(new_silver_df.visibility).over(window_spec)) \\\n                              .withColumn(\"avg_humidity\", avg(new_silver_df.humidity).over(window_spec)) \\\n                              .withColumn(\"avg_wind_deg\", avg(new_silver_df.wind_deg).over(window_spec)) \\\n                              .withColumn(\"avg_wind_gust\", avg(new_silver_df.wind_gust).over(window_spec))\n    start = datetime.fromtimestamp(new_silver_df.selectExpr(\"min(dt)\").first()[0])\n    end = datetime.fromtimestamp(new_silver_df.selectExpr(\"max(dt)\").first()[0])\n   \n    return avg_weather_data, start, end\n"],"metadata":{"application/vnd.databricks.v1+cell":{"showTitle":false,"cellMetadata":{"rowLimit":10000,"byteLimit":2048000},"nuid":"788c985d-248d-4c3e-8f47-8ac7e968ebc7","inputWidgets":{},"title":""}},"outputs":[{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"data":"\u001B[0;31m---------------------------------------------------------------------------\u001B[0m\n\u001B[0;31mNameError\u001B[0m                                 Traceback (most recent call last)\nFile \u001B[0;32m<command-3800322817382679>:1\u001B[0m\n\u001B[0;32m----> 1\u001B[0m \u001B[38;5;129m@keep_log\u001B[39m\n\u001B[1;32m      2\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21mdaily_data\u001B[39m(new_silver_df):\n\u001B[1;32m      3\u001B[0m     \u001B[38;5;28;01mfrom\u001B[39;00m \u001B[38;5;21;01mpyspark\u001B[39;00m\u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01msql\u001B[39;00m\u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01mwindow\u001B[39;00m \u001B[38;5;28;01mimport\u001B[39;00m Window\n\u001B[1;32m      4\u001B[0m     \u001B[38;5;28;01mfrom\u001B[39;00m \u001B[38;5;21;01mpyspark\u001B[39;00m\u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01msql\u001B[39;00m\u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01mfunctions\u001B[39;00m \u001B[38;5;28;01mimport\u001B[39;00m avg\n\n\u001B[0;31mNameError\u001B[0m: name 'keep_log' is not defined","errorSummary":"<span class='ansi-red-fg'>NameError</span>: name 'keep_log' is not defined","metadata":{},"errorTraceType":"ansi","type":"ipynbError","arguments":{}}},"output_type":"display_data","data":{"text/plain":["\u001B[0;31m---------------------------------------------------------------------------\u001B[0m\n","\u001B[0;31mNameError\u001B[0m                                 Traceback (most recent call last)\n","File \u001B[0;32m<command-3800322817382679>:1\u001B[0m\n","\u001B[0;32m----> 1\u001B[0m \u001B[38;5;129m@keep_log\u001B[39m\n","\u001B[1;32m      2\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21mdaily_data\u001B[39m(new_silver_df):\n","\u001B[1;32m      3\u001B[0m     \u001B[38;5;28;01mfrom\u001B[39;00m \u001B[38;5;21;01mpyspark\u001B[39;00m\u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01msql\u001B[39;00m\u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01mwindow\u001B[39;00m \u001B[38;5;28;01mimport\u001B[39;00m Window\n","\u001B[1;32m      4\u001B[0m     \u001B[38;5;28;01mfrom\u001B[39;00m \u001B[38;5;21;01mpyspark\u001B[39;00m\u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01msql\u001B[39;00m\u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01mfunctions\u001B[39;00m \u001B[38;5;28;01mimport\u001B[39;00m avg\n","\n","\u001B[0;31mNameError\u001B[0m: name 'keep_log' is not defined"]}}],"execution_count":0}],"metadata":{"application/vnd.databricks.v1+notebook":{"notebookName":"Fact Hourly and Daily table utils","dashboards":[],"notebookMetadata":{"pythonIndentUnit":4,"mostRecentlyExecutedCommandWithImplicitDF":{"commandId":3800322817382643,"dataframes":["_sqldf"]}},"language":"python","widgets":{}}},"nbformat":4,"nbformat_minor":0}
