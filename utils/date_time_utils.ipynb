{"cells":[{"cell_type":"markdown","source":["#### Defining function to load data into dim_date table"],"metadata":{"application/vnd.databricks.v1+cell":{"showTitle":false,"cellMetadata":{},"nuid":"40616856-87d3-4d03-9a51-842c6e4c3ff4","inputWidgets":{},"title":""}}},{"cell_type":"code","source":["def load_date(start_date='2023-01-01', end_date='2023-12-31'):\n    # importing necessary libraries\n    from pyspark.sql import functions as F\n    from pyspark.sql.types import IntegerType\n    from pyspark.pandas import date_range\n\n    # creating daterange\n    date_df = spark.createDataFrame(date_range(start=start_date, end=end_date).to_numpy(), ['Date'])\n    # creating other columns\n    date_df = date_df.withColumn('dateID', F.date_format('Date', 'yyyyMMdd').cast(IntegerType()))\n    date_df = date_df.withColumn('fullDate', F.date_format('Date', 'yyyy-MM-dd').cast('DATE'))\n    date_df = date_df.withColumn('monthName', F.date_format('Date', 'MMMM'))\n    date_df = date_df.withColumn('monthNumOfYear', F.month('Date'))\n    date_df = date_df.withColumn('dayNameOfWeek', F.date_format('Date', 'EEEE'))\n    date_df = date_df.withColumn('dayNumOfWeek', F.dayofweek('Date'))\n    date_df = date_df.withColumn('dayNumOfMonth', F.dayofmonth('Date'))\n    date_df = date_df.withColumn('dayNumOfYear', F.dayofyear('Date'))\n    date_df = date_df.withColumn('weekNumOfYear', F.weekofyear('Date'))\n    date_df = date_df.withColumn('quarterName', F.concat(F.lit('Q').cast('string'), F.quarter('Date')))\n    date_df = date_df.withColumn('calenderQuarter', F.quarter('Date'))\n    date_df = date_df.withColumn('calenderYear', F.year('Date'))\n    date_df = date_df.drop('Date')\n\n    # loading into dim_date table\n    date_df.write.format('delta').mode('overwrite').saveAsTable('dim_date')\n"],"metadata":{"application/vnd.databricks.v1+cell":{"showTitle":false,"cellMetadata":{"rowLimit":10000,"byteLimit":2048000},"nuid":"ce19cd54-5364-4285-b9ed-c517aab34b5a","inputWidgets":{},"title":""}},"outputs":[],"execution_count":0},{"cell_type":"markdown","source":["#### Defining function to load data into dim_time table"],"metadata":{"application/vnd.databricks.v1+cell":{"showTitle":false,"cellMetadata":{},"nuid":"72ee630b-bb90-4237-9810-aab4ca3924c8","inputWidgets":{},"title":""}}},{"cell_type":"code","source":["def load_time():\n\n    from pyspark.sql.functions import col\n\n    time_ranges = []\n    \n    for hour in range(24):\n        start_time = f\"{hour:02d}:00\"  # Format start time as HH:00\n        end_time = f\"{hour:02d}:59\"  # Format end time as HH:59\n        time_ranges.append({\"timeID\": hour, \"startTime\": start_time, \"endTime\": end_time})\n    \n    dfs = spark.createDataFrame(time_ranges)\n    dfs = dfs.withColumn('timeID', col('timeID').cast('int'))\n    dfs.write.format('delta').mode('overwrite').saveAsTable('dim_time')\n"],"metadata":{"application/vnd.databricks.v1+cell":{"showTitle":false,"cellMetadata":{"rowLimit":10000,"byteLimit":2048000},"nuid":"9d61ffc8-2ab3-4803-968f-4d1037b069b1","inputWidgets":{},"title":""}},"outputs":[],"execution_count":0}],"metadata":{"application/vnd.databricks.v1+notebook":{"notebookName":"date_time_utils","dashboards":[],"notebookMetadata":{"pythonIndentUnit":4,"mostRecentlyExecutedCommandWithImplicitDF":{"commandId":1351560671697087,"dataframes":["_sqldf"]}},"language":"python","widgets":{}}},"nbformat":4,"nbformat_minor":0}
