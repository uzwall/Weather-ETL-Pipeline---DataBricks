{"cells":[{"attachments":{},"cell_type":"markdown","metadata":{"application/vnd.databricks.v1+cell":{"cellMetadata":{},"inputWidgets":{},"nuid":"40616856-87d3-4d03-9a51-842c6e4c3ff4","showTitle":false,"title":""}},"source":["#### Defining function to load data into dim_date table"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["%sql\n","drop table if exists dim_date;\n","drop table if exists dim_time;"]},{"cell_type":"code","execution_count":null,"metadata":{"application/vnd.databricks.v1+cell":{"cellMetadata":{"byteLimit":2048000,"rowLimit":10000},"inputWidgets":{},"nuid":"ce19cd54-5364-4285-b9ed-c517aab34b5a","showTitle":false,"title":""}},"outputs":[],"source":["def load_date(start_date='2023-01-01', end_date='2023-12-31'):\n","    # importing necessary libraries\n","    from pyspark.sql import functions as F\n","    from pyspark.sql.types import IntegerType\n","    from pyspark.pandas import date_range\n","\n","    # creating daterange\n","    date_df = spark.createDataFrame(date_range(start=start_date, end=end_date).to_numpy(), ['Date'])\n","    # creating other columns\n","    date_df = date_df.withColumn('dateID', F.date_format('Date', 'yyyyMMdd').cast(IntegerType()))\n","    date_df = date_df.withColumn('fullDate', F.date_format('Date', 'yyyy-MM-dd').cast('DATE'))\n","    date_df = date_df.withColumn('monthName', F.date_format('Date', 'MMMM'))\n","    date_df = date_df.withColumn('monthNumOfYear', F.month('Date'))\n","    date_df = date_df.withColumn('dayNameOfWeek', F.date_format('Date', 'EEEE'))\n","    date_df = date_df.withColumn('dayNumOfWeek', F.dayofweek('Date'))\n","    date_df = date_df.withColumn('dayNumOfMonth', F.dayofmonth('Date'))\n","    date_df = date_df.withColumn('dayNumOfYear', F.dayofyear('Date'))\n","    date_df = date_df.withColumn('weekNumOfYear', F.weekofyear('Date'))\n","    date_df = date_df.withColumn('quarterName', F.concat(F.lit('Q').cast('string'), F.quarter('Date')))\n","    date_df = date_df.withColumn('calenderQuarter', F.quarter('Date'))\n","    date_df = date_df.withColumn('calenderYear', F.year('Date'))\n","    date_df = date_df.drop('Date')\n","\n","    # loading into dim_date table\n","    date_df.write.format('delta').mode('overwrite').saveAsTable('dim_date')\n"]},{"attachments":{},"cell_type":"markdown","metadata":{"application/vnd.databricks.v1+cell":{"cellMetadata":{},"inputWidgets":{},"nuid":"72ee630b-bb90-4237-9810-aab4ca3924c8","showTitle":false,"title":""}},"source":["#### Defining function to load data into dim_time table"]},{"cell_type":"code","execution_count":null,"metadata":{"application/vnd.databricks.v1+cell":{"cellMetadata":{"byteLimit":2048000,"rowLimit":10000},"inputWidgets":{},"nuid":"9d61ffc8-2ab3-4803-968f-4d1037b069b1","showTitle":false,"title":""}},"outputs":[],"source":["def load_time():\n","\n","    from pyspark.sql.functions import col\n","\n","    time_ranges = []\n","    \n","    for hour in range(24):\n","        start_time = f\"{hour:02d}:00\"  # Format start time as HH:00\n","        end_time = f\"{hour:02d}:59\"  # Format end time as HH:59\n","        time_ranges.append({\"timeID\": hour, \"startTime\": start_time, \"endTime\": end_time})\n","    \n","    dfs = spark.createDataFrame(time_ranges)\n","    dfs = dfs.withColumn('timeID', col('timeID').cast('int'))\n","    dfs.write.format('delta').mode('overwrite').saveAsTable('dim_time')\n"]}],"metadata":{"application/vnd.databricks.v1+notebook":{"dashboards":[],"language":"python","notebookMetadata":{"mostRecentlyExecutedCommandWithImplicitDF":{"commandId":1351560671697087,"dataframes":["_sqldf"]},"pythonIndentUnit":4},"notebookName":"date_time_utils","widgets":{}},"language_info":{"name":"python"}},"nbformat":4,"nbformat_minor":0}
